{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "b2Nj5axWqbDE",
        "l4uqhm-8izTh",
        "RI-tyq65glEb"
      ],
      "authorship_tag": "ABX9TyPomzxytiEzHNd6iRvR/vVM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yingwang/image_analysis/blob/main/Image_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image comparison in Sharpness, Luminance, Color and Noise level"
      ],
      "metadata": {
        "id": "QhZp1C5pbwPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "PdlaC12cgaSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "import dlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy import ndimage\n",
        "from skimage.metrics import structural_similarity as ssim"
      ],
      "metadata": {
        "id": "PdruIg6upwnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "b2Nj5axWqbDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img1_filename = \"image1.png\" # @param [\"image1.png\"] {allow-input: true}\n",
        "img2_filename = \"image2.png\" # @param [\"image2.png\"] {allow-input: true}\n",
        "compare_faces_only = False # @param {type:\"boolean\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "pfdyFlaef6Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read files"
      ],
      "metadata": {
        "id": "l4uqhm-8izTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "input_dir = \"/content/drive/MyDrive/input_images/\"\n",
        "output_dir = \"/content/drive/MyDrive/output_images/\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "img1_path = os.path.join(input_dir, img1_filename)\n",
        "img2_path = os.path.join(input_dir, img2_filename)\n",
        "img1 = cv2.imread(img1_path)\n",
        "img2 = cv2.imread(img2_path)\n",
        "if img1 is None or img2 is None:\n",
        "  print(\"Error: One or both images failed to load.\")"
      ],
      "metadata": {
        "id": "oGc-pJSpKYNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crop"
      ],
      "metadata": {
        "id": "RI-tyq65glEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def align_and_resize_faces(img1, img2):\n",
        "    \"\"\"Align and resize faces using facial landmarks\"\"\"\n",
        "    # Initialize dlib face detector and landmark predictor\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor(os.path.join(input_dir, 'shape_predictor_68_face_landmarks.dat')) # Model download required\n",
        "\n",
        "    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    faces1 = detector(gray1)\n",
        "    faces2 = detector(gray2)\n",
        "\n",
        "    if len(faces1) > 0 and len(faces2) > 0:\n",
        "        shape1 = predictor(gray1, faces1[0])\n",
        "        shape2 = predictor(gray2, faces2[0])\n",
        "\n",
        "        face1_aligned = align_face(img1, shape1)\n",
        "        face2_aligned = align_face(img2, shape2)\n",
        "\n",
        "        final_size = (min(face1_aligned.shape[1], face2_aligned.shape[1]),\n",
        "                      min(face1_aligned.shape[0], face2_aligned.shape[0]))\n",
        "\n",
        "        return (cv2.resize(face1_aligned, final_size),\n",
        "                cv2.resize(face2_aligned, final_size))\n",
        "    else:\n",
        "        print(\"No faces detected\")\n",
        "        return None, None\n",
        "\n",
        "def align_face(img, shape):\n",
        "    \"\"\"Align the face using landmarks\"\"\"\n",
        "    left_eye = (np.mean([shape.part(36).x, shape.part(39).x]), np.mean([shape.part(36).y, shape.part(39).y]))\n",
        "    right_eye = (np.mean([shape.part(42).x, shape.part(45).x]), np.mean([shape.part(42).y, shape.part(45).y]))\n",
        "\n",
        "    d_y = right_eye[1] - left_eye[1]\n",
        "    d_x = right_eye[0] - left_eye[0]\n",
        "    angle = np.degrees(np.arctan2(d_y, d_x))\n",
        "    center = (img.shape[1] // 2, img.shape[0] // 2)\n",
        "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    rotated = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
        "\n",
        "    left = min([shape.part(i).x for i in range(17)])\n",
        "    right = max([shape.part(i).x for i in range(17)])\n",
        "    top = min(shape.part(19).y, shape.part(24).y)\n",
        "    bottom = shape.part(8).y\n",
        "    cropped = rotated[top:bottom, left:right]\n",
        "\n",
        "    return cropped"
      ],
      "metadata": {
        "id": "N4TLllLaY8nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_and_crop(img1, img2):\n",
        "    \"\"\"Align and crop to the common area with the same size\"\"\"\n",
        "    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    sift = cv2.SIFT_create()\n",
        "    kp1, des1 = sift.detectAndCompute(gray1, None)\n",
        "    kp2, des2 = sift.detectAndCompute(gray2, None)\n",
        "\n",
        "    flann = cv2.FlannBasedMatcher(dict(algorithm=1, trees=5), dict(checks=50))\n",
        "    matches = flann.knnMatch(des1, des2, k=2)\n",
        "\n",
        "    good = [m for m, n in matches if m.distance < 0.7 * n.distance]\n",
        "\n",
        "    if len(good) >= 4:\n",
        "        # Calculate homography matrix\n",
        "        src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
        "        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
        "        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
        "\n",
        "        # Calculate the coordinates of the transformed corners of image 1\n",
        "        h, w = img1.shape[:2]\n",
        "        corners = np.float32([[0,0], [0,h], [w,h], [w,0]]).reshape(-1,1,2)\n",
        "        transformed_corners = cv2.perspectiveTransform(corners, H)\n",
        "\n",
        "        # Calculate the boundaries of the common area\n",
        "        x_min = max(0, int(min(transformed_corners[:,0,0])))\n",
        "        y_min = max(0, int(min(transformed_corners[:,0,1])))\n",
        "        x_max = min(img2.shape[1], int(max(transformed_corners[:,0,0])))\n",
        "        y_max = min(img2.shape[0], int(max(transformed_corners[:,0,1])))\n",
        "\n",
        "        # Calculate the cropping dimensions\n",
        "        crop_width = x_max - x_min\n",
        "        crop_height = y_max - y_min\n",
        "\n",
        "        # Calculate the cropping area corresponding to img1\n",
        "        M_inv = np.linalg.inv(H)\n",
        "        img1_corners = cv2.perspectiveTransform(\n",
        "            np.float32([[x_min,y_min], [x_min,y_max], [x_max,y_max], [x_max,y_min]]).reshape(-1,1,2),\n",
        "            M_inv\n",
        "        )\n",
        "        x1_min = max(0, int(np.min(img1_corners[:,0,0])))\n",
        "        y1_min = max(0, int(np.min(img1_corners[:,0,1])))\n",
        "        x1_max = min(w, int(np.max(img1_corners[:,0,0])))\n",
        "        y1_max = min(h, int(np.max(img1_corners[:,0,1])))\n",
        "\n",
        "        cropped1 = img1[y1_min:y1_max, x1_min:x1_max]\n",
        "        cropped2 = img2[y_min:y_max, x_min:x_max]\n",
        "\n",
        "        final_size = (min(cropped1.shape[1], cropped2.shape[1]),\n",
        "                     min(cropped1.shape[0], cropped2.shape[0]))\n",
        "\n",
        "        return (cv2.resize(cropped1, final_size),\n",
        "               cv2.resize(cropped2, final_size))\n",
        "\n",
        "    else:  # Use center cropping when there are insufficient features\n",
        "        h = min(img1.shape[0], img2.shape[0])\n",
        "        w = min(img1.shape[1], img2.shape[1])\n",
        "        y1 = (img1.shape[0]-h)//2\n",
        "        x1 = (img1.shape[1]-w)//2\n",
        "        y2 = (img2.shape[0]-h)//2\n",
        "        x2 = (img2.shape[1]-w)//2\n",
        "\n",
        "        return img1[y1:y1+h, x1:x1+w], img2[y2:y2+h, x2:x2+w]"
      ],
      "metadata": {
        "id": "CjPEVvF-KVvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis"
      ],
      "metadata": {
        "id": "uk7Ai3Tni3ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def color_analysis(img1, img2):\n",
        "    img1_lab = cv2.cvtColor(img1, cv2.COLOR_BGR2LAB)\n",
        "    img2_lab = cv2.cvtColor(img2, cv2.COLOR_BGR2LAB)\n",
        "\n",
        "    l1, a1, b1 = cv2.split(img1_lab)\n",
        "    l2, a2, b2 = cv2.split(img2_lab)\n",
        "    b1_rgb, g1_rgb, r1_rgb = cv2.split(img1)\n",
        "    b2_rgb, g2_rgb, r2_rgb = cv2.split(img2)\n",
        "\n",
        "    l_diff = cv2.absdiff(l1, l2)\n",
        "    lab_color_diff = np.sqrt((a1 - a2)**2 + (b1 - b2)**2).astype(np.uint8)\n",
        "\n",
        "    r_diff = cv2.absdiff(r1_rgb, r2_rgb)\n",
        "    g_diff = cv2.absdiff(g1_rgb, g2_rgb)\n",
        "    b_diff = cv2.absdiff(b1_rgb, b2_rgb)\n",
        "\n",
        "    bgr_diff = cv2.merge([b_diff, g_diff, r_diff])\n",
        "    return l_diff, lab_color_diff, bgr_diff, r_diff, g_diff, b_diff\n",
        "\n",
        "def sharpness_analysis(img1, img2):\n",
        "    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    sharpness1 = cv2.Laplacian(gray1, cv2.CV_64F).var()\n",
        "    sharpness2 = cv2.Laplacian(gray2, cv2.CV_64F).var()\n",
        "\n",
        "    sharp_diff = cv2.absdiff(\n",
        "        cv2.convertScaleAbs(cv2.Laplacian(gray1, cv2.CV_64F)),\n",
        "        cv2.convertScaleAbs(cv2.Laplacian(gray2, cv2.CV_64F))\n",
        "    )\n",
        "    return sharp_diff, sharpness1, sharpness2\n",
        "\n",
        "def denoise_analysis(img1, img2):\n",
        "    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calculate high-frequency energy\n",
        "    fft1 = np.fft.fft2(gray1)\n",
        "    fft_shift1 = np.fft.fftshift(fft1)\n",
        "    magnitude1 = 20 * np.log(np.abs(fft_shift1))\n",
        "\n",
        "    fft2 = np.fft.fft2(gray2)\n",
        "    fft_shift2 = np.fft.fftshift(fft2)\n",
        "    magnitude2 = 20 * np.log(np.abs(fft_shift2))\n",
        "\n",
        "    noise_diff = cv2.absdiff(magnitude1.astype(np.uint8), magnitude2.astype(np.uint8))\n",
        "    return noise_diff\n",
        "\n",
        "def visualize_diff(base_img, diff_map, title, threshold=30):\n",
        "    _, thresh = cv2.threshold(diff_map, threshold, 255, cv2.THRESH_BINARY)\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    vis_img = base_img.copy()\n",
        "    for cnt in contours:\n",
        "        x, y, w, h = cv2.boundingRect(cnt)\n",
        "        cv2.rectangle(vis_img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "    cv2.putText(vis_img, f\"{title} Diff\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
        "    return vis_img"
      ],
      "metadata": {
        "id": "io9BIXjJTDks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize"
      ],
      "metadata": {
        "id": "ML4xHznmjBTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_analysis(aligned_img1, aligned_img2, l_diff, bgr_diff, sharp_diff, noise_diff, sharp1, sharp2):\n",
        "    def to_float(x):\n",
        "        return float(x.mean() if isinstance(x, np.ndarray) else x)\n",
        "\n",
        "    # Luminance\n",
        "    avg_b = [to_float(np.mean(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))) for img in [aligned_img1, aligned_img2]]\n",
        "    brighter = \"(Image 1 > Image 2)\" if avg_b[0] > avg_b[1] else \"(Image 2 > Image 1)\"\n",
        "    brightness_diff = to_float(np.mean(l_diff))\n",
        "\n",
        "    # Color\n",
        "    rgb_diffs = [\n",
        "        to_float(np.mean(bgr_diff[2])),  # Red\n",
        "        to_float(np.mean(bgr_diff[1])),  # Green\n",
        "        to_float(np.mean(bgr_diff[0]))   # Blue\n",
        "    ]\n",
        "\n",
        "    # Sharpness\n",
        "    sharp_rel = \"Image 1 > Image 2\" if sharp1 > sharp2 else \"Image 2 > Image 1\"\n",
        "\n",
        "    # Noise\n",
        "    noise_values = [to_float(noise_diff[0]), to_float(noise_diff[1])]\n",
        "    noise_compare = \"Image 1 cleaner\" if noise_values[0] < noise_values[1] else \"Image 2 cleaner\"\n",
        "\n",
        "    print(f\"\"\"\n",
        "    Brightness: {avg_b[0]:.1f} vs {avg_b[1]:.1f} {brighter}\n",
        "    Sharpness: {sharp1:.1f} vs {sharp2:.1f} ({sharp_rel})\n",
        "    Noise: {noise_values[0]:.1f} vs {noise_values[1]:.1f} ({noise_compare})\n",
        "\n",
        "    Color:\n",
        "      Red:   {rgb_diffs[0]:+.1f} ({'Image 1 > Image 2' if rgb_diffs[0] > 0 else 'Image 2 > Image 1'})\n",
        "      Green: {rgb_diffs[1]:+.1f} ({'Image 1 > Image 2' if rgb_diffs[1] > 0 else 'Image 2 > Image 1'})\n",
        "      Blue:  {rgb_diffs[2]:+.1f} ({'Image 1 > Image 2' if rgb_diffs[2] > 0 else 'Image 2 > Image 1'})\n",
        "        \"\"\")"
      ],
      "metadata": {
        "id": "MtNE7UN5iRWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_and_visualize(img1, img2, compare_faces_only):\n",
        "    if compare_faces_only:\n",
        "        aligned_img1, aligned_img2 = align_and_resize_faces(img1, img2)\n",
        "    else:\n",
        "        aligned_img1, aligned_img2 = align_and_crop(img1, img2)\n",
        "\n",
        "    combined_aligned = np.hstack((aligned_img1, aligned_img2))\n",
        "    cv2.imwrite('combined.png', combined_aligned)\n",
        "    cv2_imshow(combined_aligned)\n",
        "    cv2.waitKey(0)\n",
        "\n",
        "    l_diff, lab_color_diff, rgb_diff, r_diff, g_diff, b_diff = color_analysis(aligned_img1, aligned_img2)\n",
        "    sharp_diff, sharp1, sharp2 = sharpness_analysis(aligned_img1, aligned_img2)\n",
        "    noise_diff = denoise_analysis(aligned_img1, aligned_img2)\n",
        "\n",
        "    visualizations = {\n",
        "    \"Brightness\": visualize_diff(aligned_img1, l_diff, \"Brightness\", threshold=30),\n",
        "    \"Sharpness\": visualize_diff(aligned_img1, sharp_diff, \"Sharpness\", threshold=25),\n",
        "    \"Denoise\": visualize_diff(aligned_img1, noise_diff, \"Denoise\", threshold=20),\n",
        "    \"LAB Color\": visualize_diff(aligned_img1, lab_color_diff, \"LAB Color\", threshold=20),\n",
        "    \"Red Channel\": visualize_diff(aligned_img1, r_diff, \"Red Channel\", threshold=30),\n",
        "    \"Green Channel\": visualize_diff(aligned_img1, g_diff, \"Green Channel\", threshold=30),\n",
        "    \"Blue Channel\": visualize_diff(aligned_img1, b_diff, \"Blue Channel\", threshold=30),\n",
        "    }\n",
        "\n",
        "    combined_visuals = np.vstack((\n",
        "    np.hstack((visualizations[\"Brightness\"], visualizations[\"LAB Color\"])),\n",
        "    np.hstack((visualizations[\"Sharpness\"], visualizations[\"Denoise\"])),\n",
        "    ))\n",
        "\n",
        "    cv2.imwrite('combined_diff.png', combined_visuals)\n",
        "    cv2_imshow(combined_visuals)\n",
        "    cv2.waitKey(0)\n",
        "\n",
        "    l_diff, lab_color_diff, bgr_diff, r_diff, g_diff, b_diff = color_analysis(aligned_img1, aligned_img2)\n",
        "    sharp_diff, sharp1, sharp2 = sharpness_analysis(aligned_img1, aligned_img2)\n",
        "    noise_diff = denoise_analysis(aligned_img1, aligned_img2)\n",
        "\n",
        "    print_analysis(aligned_img1, aligned_img2,\n",
        "                  l_diff, bgr_diff,\n",
        "                  sharp_diff, noise_diff, sharp1, sharp2)"
      ],
      "metadata": {
        "id": "X_JTlnouyMBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_and_visualize(img1, img2, compare_faces_only)"
      ],
      "metadata": {
        "id": "qNZRXuPpP-Tq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}